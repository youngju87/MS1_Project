{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "engaged-eugene",
   "metadata": {},
   "source": [
    "# Electoral Map Creator CA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-operations",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73285c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Versions\n",
    "# googlemaps 4.4.5\n",
    "# requests 2.25.1\n",
    "# idna 2.10\n",
    "# urlib3 1.26.4\n",
    "# chardet 4.0.0\n",
    "# pandas 1.2.4\n",
    "# xlrd 2.0.1\n",
    "# openpyxl 3.0.7\n",
    "# plotly 4.14.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a06361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import googlemaps\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Set this parameter to speed up rendering\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "pd.set_option('display.float_format',lambda x:'%.2f'%x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b36498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed8b03",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c9a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import materials and services data from Maximo and Passport databases - parse order date as date\n",
    "mx_mat = pd.read_excel('MX_Materials_NOSQL.xlsx', engine = 'openpyxl', parse_dates=[9])\n",
    "pp_mat = pd.read_excel('PP_Materials.xlsx', engine = 'openpyxl', parse_dates=[9])\n",
    "mx_serv = pd.read_excel('MX_Services_NOSQL.xlsx', engine = 'openpyxl', parse_dates=[9])\n",
    "pp_serv = pd.read_excel('PP_Services.xlsx', engine = 'openpyxl', parse_dates=[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename company id column and align Ontario and Canada nomenclature\n",
    "pp_mat = pp_mat.rename(columns = {'COMPANIESID':'COMPANYID'})\n",
    "pp_mat = pp_mat.replace(to_replace={'STATE':{'ON':'ONTARIO'},'COUNTRY':{'CA':'CAN'} })\n",
    "\n",
    "# Strip whitespace from columns\n",
    "pp_mat['CITY'] = pp_mat['CITY'].str.strip().str.upper()\n",
    "pp_mat['STATE'] = pp_mat['STATE'].str.strip().str.upper()\n",
    "pp_mat['ZIPCODE'] = pp_mat['ZIPCODE'].str.strip()\n",
    "mx_mat['CITY'] = mx_mat['CITY'].str.strip().str.upper()\n",
    "mx_mat['STATE'] = mx_mat['STATE'].str.strip().str.upper()\n",
    "mx_mat['ZIPCODE'] = mx_mat['ZIPCODE'].str.strip()\n",
    "\n",
    "# add missing status column and set to Complete (old database so all orders are complete)\n",
    "pp_mat['STATUS'] = 'COMPLETE'\n",
    "\n",
    "# concatenate the Maximo and Passport data for Material purchases\n",
    "material = pd.concat([mx_mat,pp_mat], axis=0,ignore_index=True)\n",
    "\n",
    "# rename company id column and align Ontario and Canada nomenclature\n",
    "pp_serv = pp_serv.rename(columns = {'COMPANIESID':'COMPANYID','EXTLINECOSTCAD':'LINECOST','ORDERQTY':'QTYORD'})\n",
    "pp_serv = pp_serv.replace(to_replace={'STATE':{'ON':'ONTARIO'},'COUNTRY':{'CA':'CAN'} })\n",
    "\n",
    "pp_serv['CITY'] = pp_serv['CITY'].str.strip().str.upper()\n",
    "pp_serv['STATE'] = pp_serv['STATE'].str.strip().str.upper()\n",
    "pp_serv['ZIPCODE'] = pp_serv['ZIPCODE'].str.strip()\n",
    "mx_serv['CITY'] = mx_serv['CITY'].str.strip().str.upper()\n",
    "mx_serv['STATE'] = mx_serv['STATE'].str.strip().str.upper()\n",
    "mx_serv['ZIPCODE'] = mx_serv['ZIPCODE'].str.strip()\n",
    "\n",
    "# concatenate the Maximo and Passport data for Service purchases\n",
    "service = pd.concat([mx_serv,pp_serv], axis=0,ignore_index=True)\n",
    "service = service.rename(columns = {'POSTATUS':'STATUS'})\n",
    "\n",
    "# concatenate the Maximo and Passport data for Materials and Services\n",
    "orders = pd.concat([material,service], axis=0,ignore_index=True)\n",
    "orders = orders.replace(to_replace={'SOURCE':{'MX_MATERIALS':'MATERIALS','PP_MATERIALS':'MATERIALS',\n",
    "                                             'MX_SERVICES':'SERVICES','PP_SERVICES':'SERVICES'}})\n",
    "\n",
    "# Print Total spend \n",
    "orders_total = orders['LINECOST'].sum()\n",
    "print('Total spend was ${:,.2f}'.format(orders_total))\n",
    "\n",
    "# filter for purchases in Ontario only\n",
    "Ont_df = orders[orders['STATE']=='ONTARIO']\n",
    "\n",
    "# drop order lines that don't have a proper postal code (QTY 2)\n",
    "Ont_df = Ont_df.dropna(subset = ['ZIPCODE'],axis=0)\n",
    "\n",
    "# Clean up postal codes to ensure they map properly to google API\n",
    "Ont_df['ZIPCODE'] = Ont_df['ZIPCODE'].str.replace(' ','').astype(str)\n",
    "Ont_df['ZIPCODE'] = Ont_df['ZIPCODE'].apply(lambda x:str(x[0:3]+' '+str(x[3:])))\n",
    "\n",
    "# Set Company ID to be string type for plotting\n",
    "Ont_df['COMPANYID'] = Ont_df['COMPANYID'].map(str)\n",
    "\n",
    "# Group by company and aggregate by spend\n",
    "Ont_comp_spend = Ont_df.groupby(['COMPANYID','CITY','ZIPCODE','SOURCE'])['LINECOST'].sum().reset_index(name='SPEND')\n",
    "\n",
    "# Print Total spend in Ontario\n",
    "Ontario_total = Ont_comp_spend['SPEND'].sum()\n",
    "print('Total spend in Ontario was ${:,.2f}'.format(Ontario_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4404f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontario spend over time\n",
    "Ont_spend_year = Ont_df[['ORDERDT','LINECOST']].set_index('ORDERDT').resample('Y').sum().reset_index()\n",
    "Ont_spend_year['YEAR'] = Ont_spend_year['ORDERDT'].dt.year\n",
    "\n",
    "fig = px.bar(Ont_spend_year,x='YEAR',y='LINECOST')\n",
    "# # Updating the laout for titles, axis labels, and legend\n",
    "fig.update_layout(title='Total Nuclear Spend in Ontario by Year',\n",
    "                  margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0},\n",
    "                  title_font_size=20,\n",
    "                  title_yanchor='top',\n",
    "                  title_pad = dict(t=10,b=10),\n",
    "                  xaxis_title=\"Year\", \n",
    "                  yaxis_title=\"Spend, $CAD\",\n",
    "                  \n",
    "                  )\n",
    "fig.update_xaxes(\n",
    "    tickmode=\"linear\",\n",
    "    tick0=2018,\n",
    "    dtick=\"Y\",\n",
    "    tickformat=\"Y\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7560a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ont_source = Ont_df.groupby(['SOURCE'])['LINECOST'].sum().reset_index(name='SPEND')\n",
    "\n",
    "fig = px.bar(Ont_source,x='SOURCE',y='SPEND')\n",
    "# # Updating the laout for titles, axis labels, and legend\n",
    "fig.update_layout(title='Spend by type in Ontario',\n",
    "                  margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0},\n",
    "                  title_font_size=20,\n",
    "                  title_yanchor='top',\n",
    "                  title_pad = dict(t=10,b=10),\n",
    "                  xaxis_title=\"Spend type\", \n",
    "                  yaxis_title=\"Spend, $CAD\",\n",
    "                  \n",
    "                  )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ont_comp_spend_sort = Ont_df.groupby('COMPANYID',as_index=False)['LINECOST'].sum().sort_values('LINECOST',ascending=False)\n",
    "Top_10_suppliers = list(Ont_comp_spend_sort['COMPANYID'][0:9])\n",
    "\n",
    "fig = px.bar(Ont_comp_spend_sort[0:9],x='COMPANYID',y='LINECOST')\n",
    "# # Updating the laout for titles, axis labels, and legend\n",
    "fig.update_layout(title='Top 10 Companies by Spend in Ontario',\n",
    "                  margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0},\n",
    "                  title_font_size=20,\n",
    "                  title_yanchor='top',\n",
    "                  title_pad = dict(t=10,b=10),\n",
    "                  xaxis_title=\"CompanyID\", \n",
    "                  yaxis_title=\"Spend, $CAD\",\n",
    "                  \n",
    "                  )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ont_comp_spend_source = Ont_df.groupby(['COMPANYID','SOURCE'],as_index=False)['LINECOST'].sum().sort_values('LINECOST',ascending=False)\n",
    "Ont_comp_spend_source = Ont_comp_spend_source[Ont_comp_spend_source['COMPANYID'].isin(Top_10_suppliers)]\n",
    "\n",
    "fig = px.bar(Ont_comp_spend_source,x='COMPANYID',y='LINECOST',color='SOURCE')\n",
    "# # Updating the laout for titles, axis labels, and legend\n",
    "fig.update_layout(title='Top 10 Companies by Spend type in Ontario',\n",
    "                  margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0},\n",
    "                  title_font_size=20,\n",
    "                  title_yanchor='top',\n",
    "                  title_pad = dict(t=10,b=10),\n",
    "                  xaxis_title=\"CompanyID\", \n",
    "                  yaxis_title=\"Spend, $CAD\",\n",
    "                  \n",
    "                  )\n",
    "fig.update_xaxes({'categoryarray':Top_10_suppliers})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-possession",
   "metadata": {},
   "source": [
    "### Convert Addresses to LAT & LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Addresses\n",
    "#df_test = pd.read_excel('assets/CAN_Post-Codes.xlsx')\n",
    "#df_test.head()\n",
    "\n",
    "df = Ont_df[['CITY','ZIPCODE','STATE']].drop_duplicates().reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-enemy",
   "metadata": {},
   "source": [
    "#### Aquire Map Data (Google Maps API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Maps API Key\n",
    "gmaps_key = \"AIzaSyDiv0KJTmzrpVrLmBhYDqnAJyKNfl4pdt0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the responses will be stored\n",
    "response_object = {}\n",
    "response_object['ONTARIO'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through df\n",
    "for i in range(0, len(df)):\n",
    "\n",
    "    # Error handling\n",
    "    try:\n",
    "        print('Requesting row #:', i)\n",
    "\n",
    "        # Define the request parameters\n",
    "        postCode_address = df.iloc[i]['CITY'].replace(\" \", \"+\")+\"+\"+df.loc[i]['STATE']+\"+\"+df.loc[i]['ZIPCODE'].replace(\" \", \"+\")\n",
    "        place = df.iloc[i]['STATE']\n",
    "        \n",
    "        baseUrl = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "                \n",
    "        contents = urllib.request.urlopen(\n",
    "            baseUrl + '?' + 'address={}&key={}'\\\n",
    "            .format(postCode_address,gmaps_key)\n",
    "        ).read().decode('UTF-8')\n",
    "\n",
    "        # Converts to json format\n",
    "        contents_json = json.loads(contents)\n",
    "\n",
    "        # Insert returned json response into response_object\n",
    "        response_object[place][postCode_address] = contents_json\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        print('Returning empty response for post code:', postCode_address)\n",
    "        response_object[place][postCode_address] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name_1 = \"assets/{}-response.json\".format(datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\"))\n",
    "\n",
    "with open(f_name_1,\"w\") as outfile:\n",
    "    json.dump(response_object, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-retrieval",
   "metadata": {},
   "source": [
    "##### Create Dataframe with Map Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the place type\n",
    "place = 'ONTARIO'\n",
    "\n",
    "df_field_responses = pd.DataFrame(\n",
    "    columns=['Post_Code',\n",
    "            'City',\n",
    "            'State',\n",
    "            'Latitude',\n",
    "            'Longitude'\n",
    "            ])\n",
    "\n",
    "for (postCode_address, i) in zip(\n",
    "    response_object[place].keys(),\n",
    "    range(0, len(response_object[place]))\n",
    "):\n",
    "    \n",
    "    try:\n",
    "        print('Trying to insert response for Post Code:', postCode_address)\n",
    "        \n",
    "        # City\n",
    "        df_field_responses.loc[i, 'City'] =\\\n",
    "            response_object[place][postCode_address]['results'][0]['address_components'][1]['long_name']\n",
    "        \n",
    "        # State\n",
    "        df_field_responses.loc[i, 'State'] =\\\n",
    "            place  \n",
    "        \n",
    "        # Post Code\n",
    "        df_field_responses.loc[i, 'Post_Code'] =\\\n",
    "            re.findall(r\".{7}$\",postCode_address)[0]\n",
    "        \n",
    "        # Latitude\n",
    "        df_field_responses.loc[i, 'Latitude'] =\\\n",
    "            response_object[place][postCode_address]['results'][0]['geometry']['location']['lat']\n",
    "        \n",
    "        # Longitude\n",
    "        df_field_responses.loc[i, 'Longitude'] =\\\n",
    "            response_object[place][postCode_address]['results'][0]['geometry']['location']['lng']\n",
    "\n",
    "        print('Inserted for row {}: {}'.format(i, df_field_responses.loc[i]))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        print('Filling row with Error for row: {}; Post Code Address: {}'.format(i, postCode_address))\n",
    "        # Fill in 'Error' for row if a field couldn't be found\n",
    "        df_field_responses.loc[i] = ['Error' for i in range(0, len(df_field_responses.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the '+' with a blank space so we can turn into numbers\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df_field_responses['Post_Code'] = df_field_responses.Post_Code.str.replace(\"+\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-service",
   "metadata": {},
   "source": [
    "### Convert Lat and Long to CA Electoral Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_object2 = {}\n",
    "response_object2['ONTARIO'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-imaging",
   "metadata": {},
   "source": [
    "#### Aquire Electoral Data from Open North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through df\n",
    "for i in range(0, len(df)):\n",
    "\n",
    "    # Error handling\n",
    "    try:\n",
    "        print('Requesting row #:', i)\n",
    "\n",
    "        # Define the request parameters\n",
    "        postCode = df_field_responses.iloc[i]['Post_Code']\n",
    "        latitude = df_field_responses.iloc[i]['Latitude']\n",
    "        longitude = df_field_responses.iloc[i]['Longitude']\n",
    "        \n",
    "        # Making request\n",
    "        contents = urllib.request.urlopen(\n",
    "            'https://represent.opennorth.ca/boundaries/?contains={},{}'\\\n",
    "            .format(latitude, longitude)\n",
    "        ).read().decode('UTF-8')\n",
    "\n",
    "        # Converts to json format\n",
    "        contents_json = json.loads(contents)\n",
    "\n",
    "        # Insert returned json response into response_object\n",
    "        response_object2[place][postCode] = contents_json\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        print('Returning empty response for url:', postCode)\n",
    "        response_object2[place][postCode] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name2 ='assets/{}-response2.json'.format(datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\"))\n",
    "\n",
    "with open(f_name2, 'w') as outfile:\n",
    "    json.dump(response_object2, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-alert",
   "metadata": {},
   "source": [
    "##### Create Dataframe with Electoral Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_field_responses2 = pd.DataFrame(\n",
    "    columns=['Post_Code',\n",
    "            'Electoral_District'\n",
    "            ])\n",
    "\n",
    "for (postCode, i) in zip(\n",
    "    response_object2[place].keys(),\n",
    "    range(0, len(response_object2[place]))\n",
    "):\n",
    "    \n",
    "    try:\n",
    "        print('Trying to insert response for Post Code:', postCode)\n",
    "        \n",
    "        # Address\n",
    "        df_field_responses2.loc[i, 'Post_Code'] = postCode\n",
    "\n",
    "        # Electoral_District    \n",
    "        df_field_responses2.loc[i, 'Electoral_District'] =\\\n",
    "            response_object2[place][postCode]['objects'][1]['name']\n",
    "\n",
    "        print('Inserted for row {}: {}'.format(i, df_field_responses2.loc[i]))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        print('Filling row with Error for row: {}; Post Code: {}'.format(i, postCode))\n",
    "        # Fill in 'Error' for row if a field couldn't be found\n",
    "        df_field_responses2.loc[i] = ['Error' for i in range(0, len(df_field_responses2.columns))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-yesterday",
   "metadata": {},
   "source": [
    "#### Merge DFs for Working DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_field_responses2.drop([0])\n",
    "df_final = df_field_responses.merge(df_field_responses2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-arthur",
   "metadata": {},
   "source": [
    "#### Mapping Reps to Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-unemployment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hoc_data = pd.read_csv('assets/house-of-commons.csv', encoding ='cp1252')\n",
    "\n",
    "# Mapping Data\n",
    "df_final['Name'] = df_final['Electoral_District'].map(hoc_data.set_index('District name')['Name'])\n",
    "df_final['Email'] = df_final['Electoral_District'].map(hoc_data.set_index('District name')['Email'])\n",
    "df_final['House of Commons Phone'] = df_final['Electoral_District'].map(hoc_data.set_index('District name')['Phone'])\n",
    "df_final['Party'] = df_final['Electoral_District'].map(hoc_data.set_index('District name')['Party name'])\n",
    "df_final['Gender'] = df_final['Electoral_District'].map(hoc_data.set_index('District name')['Gender'])\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-shareware",
   "metadata": {},
   "source": [
    "## Merge Mapping Data to Source Data (Ont_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Merge with Ont_df\n",
    "df_final.rename(columns={\"Post_Code\": \"ZIPCODE\"},inplace=True)\n",
    "\n",
    "# File Test Storage\n",
    "df_final.to_excel(\"assets/df_final.xlsx\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Mapping df and Ont_df\n",
    "df_combo = Ont_df.merge(df_final, how=\"left\", on=\"ZIPCODE\")\n",
    "df_combo.to_excel(\"assets/df_combo.xlsx\") \n",
    "df_combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for Missing Data\n",
    "\n",
    "cols = df_combo.columns[:]\n",
    "colors = ['#000099', '#ffff00']\n",
    "sns.heatmap(df_combo[cols].isnull(), cmap=sns.color_palette(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b157285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing in the ShapeFile for Federal Districts\n",
    "\n",
    "geodf_fed = gpd.read_file('assets/federal_mapping_can/lfed000b16a_e.shp')\n",
    "geodf_can = gpd.read_file('assets/canada_mapping/gpr_000b11a_e.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting ShapeFile to GeoJSON\n",
    "\n",
    "geodf_fed.to_file(\"assets/federal_mapping_can/fed_geojson.geojson\", driver = \"GeoJSON\")\n",
    "with open(\"assets/federal_mapping_can/fed_geojson.geojson\") as geofile_fed:\n",
    "    geojson_fed_file = json.load(geofile_fed)\n",
    "    \n",
    "geodf_can.to_file(\"assets/canada_mapping/can_geojson.geojson\", driver = \"GeoJSON\")\n",
    "with open(\"assets/canada_mapping/can_geojson.geojson\") as geofile_can:\n",
    "    geojson_can_file = json.load(geofile_can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping fips to dataframe\n",
    "\n",
    "fed_df_geojson = pd.json_normalize(geojson_fed_file[\"features\"])\n",
    "\n",
    "# Clean & Map\n",
    "\n",
    "fed_df_geojson.columns = ['type','properties.FEDUID','properties.FEDNAME','properties.FEDENAME','properties.FEDFNAME','properties.PRUID','properties.PRNAME','geometry.type','geometry.coordinates']\n",
    "fed_df_geojson['properties.FEDENAME'] = fed_df_geojson['properties.FEDENAME'].str.replace('--','â€”')\n",
    "df_final['fips'] = df_final['Electoral_District'].map(fed_df_geojson.set_index('properties.FEDENAME')['properties.FEDUID']).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING MAP\n",
    "df_final['value'] = np.random.choice([1, 9, 20], df_final.shape[0])\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_test = df_final[['fips','value']].copy()\n",
    "df_final_test = df_final_test.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-grove",
   "metadata": {},
   "source": [
    "### Map Electoral Districts & Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STILL UNDER CONSTRUCTION\n",
    "\n",
    "fig = px.choropleth(df_final_test, geojson=fed_df_geojson, locations='fips', color='value',\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           range_color=(0, 12),\n",
    "                           scope=\"north america\",\n",
    "                           labels={'values':'value rate'}\n",
    "                          )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-daily",
   "metadata": {},
   "source": [
    "## Record Dependecies\n",
    "Below are the dependecies for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -v -m -p googlemaps,requests,idna,chardet,pandas,xlrd,openpyxl,plotly,geopandas,numpy,json,re,seaborn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
